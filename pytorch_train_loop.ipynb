{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02215aa2-4d70-4d7d-a53c-1a47cf06e004",
   "metadata": {},
   "source": [
    "Here's we'll take our building blocks from the previous notebooks and build a preliminary training loop with pytorch\n",
    "\n",
    "*some things to consider:*<br>\n",
    "(0) We are not using sensible parameters in our model instantiation, nor will we invoke the full effnet model yet <br>\n",
    "(1) Based on (0) we are not expecting the model to learn anything at this point<br>\n",
    "(2) the goal of this notebook is to have a preliminary pipeline in place --> this way we can play around with various parameters within our model and see how they impact the ability of a model to learn something useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0355f08-7aca-4695-8f56-8eaced471313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "from exabiome.nn.loader import read_dataset, LazySeqDataset\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from model import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5a275b-d653-4d3b-852e-9fb946f0b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace(**{'load': False,\n",
    "                                'window': 4096,\n",
    "                                'step': 4096,\n",
    "                                'classify': True,\n",
    "                                'tgt_tax_lvl': \"phylum\",\n",
    "                                'fwd_only': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7de44525-f09c-464a-8609-9464a0c99081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_dl(hparams, batch_size=16):\n",
    "    path = '/global/homes/a/azaidi/ar122_r202.toy.input.h5'\n",
    "    chunks = LazySeqDataset(hparams, path=path,\n",
    "                           keep_open=True)\n",
    "    ds = taxon_ds(chunks, old_pad_seq)\n",
    "    return DataLoader(ds, batch_size=batch_size, \n",
    "                      shuffle=True)#, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7db8522-589b-4341-a0ae-01d26990de68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1189, torch.Size([16, 1, 4096]), torch.Size([16]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = get_toy_dl(hparams)\n",
    "batch = next(iter(dl))\n",
    "len(dl), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2996dcf-7135-4901-ab56-098900a68a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(\n",
    "    get_base_layer(),\n",
    "    get_dep_sep(32,16),\n",
    "    get_inv_res(16, 12),\n",
    "    get_head_layer(12, 1,\n",
    "                  lin_out_feats=12)\n",
    ")(batch[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd662575-076e-4bd0-abc0-e6d5a300f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        get_base_layer(),\n",
    "        get_dep_sep(32,16),\n",
    "        get_inv_res(16, 12),\n",
    "        get_head_layer(12, 1,\n",
    "                    lin_out_feats=18))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27820f-743b-4061-9c44-360609c1fc43",
   "metadata": {},
   "source": [
    "We just want to make sure a loss function works for now -- this dataset only has 18 potential classes, so we select 18 out features in the model definition above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3987c829-9d51-4905-98a2-d191fd188f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 18]), torch.Size([16]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_model()\n",
    "out = m(batch[0]).squeeze(1)\n",
    "out.shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "315d1d59-41e1-4c33-b439-874d15d22fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0212, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()(out, batch[1])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e332a-9457-4e4f-bb2a-e618b233f0e3",
   "metadata": {},
   "source": [
    "Looks like our loss function works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3710d-f0bc-477f-8ae6-ef15ee57d841",
   "metadata": {},
   "source": [
    "This call below determines if we have a GPU available -- if so, we will want to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8d16a5a-ded0-4c8b-b19c-582fdbf99e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ebda0-4dbc-4314-9364-5196ff19a4dd",
   "metadata": {},
   "source": [
    "We will need to update the gradients after our backward pass -- we could do this manually but it would be better to use one of pytorch's optimizers, we'll go with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3444be2f-66e8-4321-b245-976ae63ab62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adam(m.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379d171-48b2-45bc-a286-70afb4d5583b",
   "metadata": {},
   "source": [
    "# Preliminary training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7bd601cb-98b0-4176-98db-82e0469b8dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9753, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9210, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1081, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0681, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8755, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9873, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9198, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9386, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2251, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9081, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = get_model()\n",
    "loss_fxn = nn.CrossEntropyLoss()\n",
    "dl = get_toy_dl(hparams)\n",
    "\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
    "opt = torch.optim.Adam(m.parameters())\n",
    "m.to(device)\n",
    "i = 0\n",
    "\n",
    "for x, y in dl:\n",
    "#    x, y = batch\n",
    "    out = m(x.to(device))\n",
    "    loss = loss_fxn(out.squeeze(1), y.to(device))\n",
    "\n",
    "    loss.backward() #pytorch computes the gradients for us\n",
    "    opt.step() #out optimizer does the weight updates for us\n",
    "    opt.zero_grad() #this could be moved to the start of for loop\n",
    "    \n",
    "    #this is just for debugging purposes + to see loss value as we train\n",
    "    if(i == 10): \n",
    "        break\n",
    "    else:\n",
    "        print(loss)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbeeacb-22e8-40b9-b295-f20d55fba3ff",
   "metadata": {},
   "source": [
    "That's our training loop - pretty simple! It's basically 6 lines of code\n",
    "\n",
    "\n",
    "for x, y in dl:\n",
    ">   out = m(x.to(device)) <br>\n",
    "    loss = loss_fxn(out.squeeze(1), y.to(device)) <br>\n",
    "    loss.backward() <br>\n",
    "    opt.step() <br>\n",
    "    opt.zero_grad() <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d09c7d-f7c5-469d-8a0a-174e101d9cd9",
   "metadata": {},
   "source": [
    "As stated before, this model will not be learning much of value with the way it's been setup + parameterized -- but this simple model is small enough to work on a pipeline with a cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134edf9-ff91-464b-8164-6c47ce7e63fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0619889-4405-4685-9b38-e8ef9c345456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zenv",
   "language": "python",
   "name": "zenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
